{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN algorithm (K-Nearest Neighbor) simplified. It can be used for both classification and \n",
    "# regression predictive problems. For simplicity, IRIS dataset is used to test the code. \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_absolute_error as MEA # I imported this but will try to not to use it.\n",
    "Iris = datasets.load_iris()\n",
    "print(type(Iris.data))\n",
    "print(Iris.data.shape)\n",
    "\n",
    "# check the length of the data, shuffle the data, prepare the test and train sets\n",
    "# it is normally good to split the data to 70-30 but our KNN does close to 100 percent accuracy in that case\n",
    "# the data is intentionally split 66-33 to show some errors for overfitting and underfitting\n",
    "# X is used for the data and y is used for labels\n",
    "length_of_data = len(Iris.data)\n",
    "print(length_of_data)\n",
    "indices = np.random.permutation(len(Iris.data))\n",
    "X_train = Iris.data[indices[:100]]\n",
    "y_train = Iris.target[indices[:100]]\n",
    "X_test = Iris.data[indices[100:]]\n",
    "y_test = Iris.target[indices[100:]]\n",
    "\n",
    "# view the data\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])\n",
    "\n",
    "# test the normal distance function to get a scalar distance value in between instances\n",
    "print(np.linalg.norm(X_train[3] - X_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Nearest_Neighbor(test_set, test_labels, training_set, training_labels, n): \n",
    "    results = []\n",
    "    for i in range(len(test_set)):\n",
    "        indices_visited = [] # make a list of indices visited\n",
    "        votes = []\n",
    "        for j in range(n):\n",
    "            min = np.inf\n",
    "            for k in range(len(training_set)):\n",
    "                difference = np.linalg.norm(training_set[k] - test_set[i])\n",
    "                if difference < min and k not in indices_visited:\n",
    "                    min = difference\n",
    "                    index_at_min_distance = k\n",
    "                    the_label_found_at_min_dist = training_labels[k]\n",
    "            indices_visited.append(index_at_min_distance)\n",
    "            votes.append(the_label_found_at_min_dist)\n",
    "        \n",
    "        # count the votes\n",
    "        a_dict = {}\n",
    "        for i in votes:\n",
    "            if i in a_dict:\n",
    "                a_dict[i] += 1\n",
    "            else:\n",
    "                a_dict[i] = 1\n",
    "        #print(a_dict)\n",
    "        # find the maximum seen vote\n",
    "        max_v = -np.inf\n",
    "        for k,v in a_dict.items():\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "                set_k = int(k)\n",
    "        # the most common vote is the label of the prediction\n",
    "        results.append(set_k) \n",
    "    #print(results, test_labels)\n",
    "    # check the accuracy by comparing our target and actual test labels\n",
    "    accuracy = np.sum(results == test_labels) / len(test_labels)\n",
    "    # print('the actual results are: {}'.format(test_labels))\n",
    "    # print('our labels show: {}'.format(results))\n",
    "    # print('the accuracy is: {}'.format(accuracy*100))\n",
    "    return np.around(100*accuracy, decimals = 2)\n",
    "    \n",
    "\n",
    "# print(K_Nearest_Neighbor(X_test, y_test, X_train, y_train, 1))\n",
    "all_results = []   \n",
    "for i in range(1,30):\n",
    "    each_result = K_Nearest_Neighbor(X_test, y_test, X_train, y_train, i)\n",
    "    all_results.append(each_result)\n",
    "print(all_results, len(all_results))\n",
    "plt.plot(np.arange(len(all_results)), np.array(all_results))\n",
    "\n",
    "# Conclusion: KNN 8-10 gives the best accuracy for this training and test sets. \n",
    "# It is slightly hard to see the overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
